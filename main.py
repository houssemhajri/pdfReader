from fastapi import FastAPI, UploadFile, File, Form,Request
from fastapi.middleware.cors import CORSMiddleware
from utils import process_pdf, search_similar_chunks, ask_llm
import os

app = FastAPI()

# Autoriser CORS pour le frontend Angular
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:4200"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Variables globales
documents = []
index = None
embeddings_model = None
texts = []

@app.post("/upload")
async def upload_pdf(file: UploadFile = File(...)):
    global documents, index, embeddings_model, texts
    file_path = f"temp_{file.filename}"
    with open(file_path, "wb") as f:
        f.write(await file.read())

    texts, index, embeddings_model = process_pdf(file_path)
    os.remove(file_path)
    return {"message": "PDF trait√© avec succ√®s."}

@app.post("/ask")
async def ask_question(question: str = Form(...)):
    print("‚úÖ Requ√™te re√ßue ! Question :", question)
    if not index:
        print("‚ö†Ô∏è Aucun PDF n'a √©t√© charg√©.")
        return {"answer": "Aucun document n'a encore √©t√© charg√©."}

    context = search_similar_chunks(question, index, embeddings_model, texts)
    print("üß† Contexte trouv√© :", context[:100])
    answer = ask_llm(question, context)
    print("üí¨ R√©ponse g√©n√©r√©e :", answer)
    return {"answer": answer}

@app.post("/ask-stream")
async def ask_stream(request: Request):
    body = await request.json()
    question = body.get("question")
    context = body.get("context")

    async def stream_response():
        response = openai.ChatCompletion.create(
            model="llama3-8b-8192",
            messages=[
                {"role": "system", "content": "Tu es un assistant qui r√©pond uniquement √† partir du contexte."},
                {"role": "user", "content": f"Contexte :\n{context}\n\nQuestion : {question}"}
            ],
            stream=True,
        )

        try:
            for chunk in response:
                delta = chunk["choices"][0]["delta"]
                if "content" in delta:
                    yield delta["content"]
                    await asyncio.sleep(0)  # permet √† FastAPI de stream
        except Exception as e:
            yield f"\n[Erreur] {str(e)}"

    return StreamingResponse(stream_response(), media_type="text/plain")